# migrations/ocr_migration.py

# ... (imports and other code remain the same) ...
import asyncio
import json
import argparse
import sys
import os
import logging

try:
    from elasticsearch import AsyncElasticsearch, NotFoundError, BadRequestError, ConnectionError as ESConnectionError
except ModuleNotFoundError:
    print("ERROR: 'elasticsearch' module not found. Please run: pip install \"elasticsearch[async]\"")
    sys.exit(1)

ROOT_FOLDER = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
sys.path.insert(0, ROOT_FOLDER)

try:
    from app.core.settings import ElasticsearchSettings
except ModuleNotFoundError:
    print("ERROR: Could not import ElasticsearchSettings. Make sure 'pydantic-settings' is installed and the app structure is correct.")
    sys.exit(1)
except ImportError:
    print("ERROR: ImportError while importing ElasticsearchSettings. Check dependencies.")
    sys.exit(1)

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- H√†m create_index_with_vi_analyzer (gi·ªØ nguy√™n) ---
async def create_index_with_vi_analyzer(client: AsyncElasticsearch, index_name: str):
    # ... (code for creating index remains the same) ...
    if not index_name:
        raise ValueError("T√™n ch·ªâ m·ª•c kh√¥ng ƒë∆∞·ª£c r·ªóng")
    if not index_name.islower() or any(c in index_name for c in r'\/*?"<>|, #'):
        raise ValueError(f"T√™n ch·ªâ m·ª•c '{index_name}' kh√¥ng h·ª£p l·ªá (ph·∫£i l√† ch·ªØ th∆∞·ªùng, kh√¥ng ch·ª©a k√Ω t·ª± ƒë·∫∑c bi·ªát).")

    logger.info(f"Ki·ªÉm tra s·ª± t·ªìn t·∫°i c·ªßa ch·ªâ m·ª•c: '{index_name}'")
    index_exists = False
    try:
        await client.indices.get(index=index_name, ignore=[404])
        if await client.indices.exists(index=index_name):
             index_exists = True
             logger.info(f"Ch·ªâ m·ª•c '{index_name}' ƒë√£ t·ªìn t·∫°i.")
        else:
             logger.warning(f"client.indices.get kh√¥ng b√°o l·ªói NotFound nh∆∞ng client.indices.exists tr·∫£ v·ªÅ false cho index '{index_name}'. Coi nh∆∞ ch∆∞a t·ªìn t·∫°i.")
             index_exists = False
    except NotFoundError:
        logger.info(f"Ch·ªâ m·ª•c '{index_name}' kh√¥ng t·ªìn t·∫°i.")
        index_exists = False
    except BadRequestError as e:
        logger.error(f"L·ªói Bad Request khi ki·ªÉm tra ch·ªâ m·ª•c '{index_name}': Status={e.status_code}, Info={e.info}, Meta={e.meta}")
        try:
             error_body = await e.body() if hasattr(e, 'body') and callable(e.body) else str(e)
             logger.error(f"Response Body: {error_body}")
        except Exception:
             logger.error("Could not retrieve error body from BadRequestError.")
        raise
    except Exception as e:
        logger.error(f"L·ªói kh√¥ng x√°c ƒë·ªãnh khi ki·ªÉm tra ch·ªâ m·ª•c '{index_name}': {e}", exc_info=True)
        raise

    if index_exists:
        try:
            logger.warning(f"ƒêang x√≥a ch·ªâ m·ª•c ƒë√£ t·ªìn t·∫°i: '{index_name}'...")
            await client.indices.delete(index=index_name, ignore=[400, 404])
            logger.info(f"ƒê√£ x√≥a ch·ªâ m·ª•c '{index_name}'.")
        except Exception as e:
            logger.error(f"L·ªói khi x√≥a ch·ªâ m·ª•c '{index_name}': {e}", exc_info=True)
            raise

    index_body = {
        "settings": {
            "analysis": {
                "analyzer": {"vi_analyzer": {"tokenizer": "vi_tokenizer", "filter": ["lowercase", "vi_stop"]}},
                "tokenizer": {"vi_tokenizer": {"type": "vi_tokenizer", "keep_punctuation": False, "split_url": False}},
                "filter": {"vi_stop": {"type": "vi_stop", "stopwords": "_vi_"}}
            }
        },
        "mappings": {"properties": {"text": {"type": "text", "analyzer": "vi_analyzer"}}}
    }

    logger.info(f"ƒêang t·∫°o ch·ªâ m·ª•c '{index_name}' v·ªõi analyzer ti·∫øng Vi·ªát...")
    try:
        await client.indices.create(index=index_name, body=index_body)
        logger.info(f"ƒê√£ t·∫°o th√†nh c√¥ng ch·ªâ m·ª•c '{index_name}'.")
    except BadRequestError as e:
        logger.error(f"L·ªói Bad Request khi t·∫°o ch·ªâ m·ª•c '{index_name}': Status={e.status_code}, Info={e.info}")
        try:
             error_body = await e.body() if hasattr(e, 'body') and callable(e.body) else str(e)
             logger.error(f"Response Body: {error_body}")
        except Exception:
             logger.error("Could not retrieve error body from BadRequestError.")
        logger.error(f"Request Body g·ª≠i ƒëi:\n{json.dumps(index_body, indent=2)}")
        raise
    except Exception as e:
        logger.error(f"L·ªói kh√¥ng x√°c ƒë·ªãnh khi t·∫°o ch·ªâ m·ª•c '{index_name}': {e}", exc_info=True)
        logger.error(f"Request Body ƒë√£ g·ª≠i:\n{json.dumps(index_body, indent=2)}")
        raise


async def migrate_ocr_data(file_path: str, settings: ElasticsearchSettings):
    """ƒê·ªçc file JSON v√† bulk index v√†o Elasticsearch."""
    logger.info("Kh·ªüi t·∫°o k·∫øt n·ªëi ƒë·∫øn Elasticsearch...")
    connection_error = None

    # --- üëá FIX: Include scheme in hosts, remove scheme= argument üëá ---
    async with AsyncElasticsearch(
        hosts=[f"http://{settings.ES_HOST}:{settings.ES_PORT}"], # Add http:// here
        http_auth=(settings.ES_USER, settings.ELASTIC_PASSWORD),
        # scheme="http", # REMOVE THIS LINE
        verify_certs=False,
        ssl_show_warn=False,
        request_timeout=60
    ) as client:
    # --- END FIX ---

        try:
            logger.info("ƒêang ki·ªÉm tra k·∫øt n·ªëi Elasticsearch b·∫±ng client.info()...")
            info = await client.info()
            logger.info(f"K·∫øt n·ªëi Elasticsearch th√†nh c√¥ng. Cluster: {info.get('cluster_name')}, Version: {info.get('version', {}).get('number')}")
        except ESConnectionError as e:
            connection_error = f"L·ªói k·∫øt n·ªëi ƒë·∫øn Elasticsearch: {e}"
        except BadRequestError as e:
             logger.error(f"L·ªói Bad Request khi g·ªçi client.info(): Status={e.status_code}, Info={e.info}")
             connection_error = f"L·ªói Bad Request khi ki·ªÉm tra k·∫øt n·ªëi (client.info): {e}"
        except Exception as e:
            connection_error = f"L·ªói kh√¥ng x√°c ƒë·ªãnh khi ki·ªÉm tra k·∫øt n·ªëi Elasticsearch: {e}"

        if connection_error:
             logger.critical(connection_error)
             raise ConnectionError(connection_error)

        await create_index_with_vi_analyzer(client, settings.ES_OCR_INDEX)

        # ... (rest of the data loading and bulk indexing code remains the same) ...
        logger.info(f"ƒêang t·∫£i d·ªØ li·ªáu OCR t·ª´: {file_path}...")
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            if not isinstance(data, dict):
                 raise TypeError("ƒê·ªãnh d·∫°ng file JSON kh√¥ng ƒë√∫ng, c·∫ßn l√† m·ªôt dictionary.")
        except FileNotFoundError:
            logger.error(f"L·ªói: Kh√¥ng t√¨m th·∫•y file JSON t·∫°i '{file_path}'")
            return
        except json.JSONDecodeError as e:
             logger.error(f"L·ªói: File JSON kh√¥ng h·ª£p l·ªá t·∫°i '{file_path}': {e}")
             return
        except Exception as e:
             logger.error(f"L·ªói kh√¥ng x√°c ƒë·ªãnh khi ƒë·ªçc file JSON: {e}", exc_info=True)
             return

        operations = []
        for key, value in data.items():
            doc_id = str(key)
            doc_text = str(value) if value is not None else ""
            operations.append({"index": {"_index": settings.ES_OCR_INDEX, "_id": doc_id}})
            operations.append({"text": doc_text})

        if not operations:
            logger.warning("Kh√¥ng c√≥ d·ªØ li·ªáu OCR n√†o ƒë·ªÉ index.")
            return

        logger.info(f"B·∫Øt ƒë·∫ßu index {len(data)} documents v√†o '{settings.ES_OCR_INDEX}'...")
        try:
            response = await client.bulk(operations=operations, request_timeout=120)

            if response.get('errors', False):
                error_count = 0
                logger.error("L·ªói x·∫£y ra trong qu√° tr√¨nh bulk indexing:")
                for i, item in enumerate(response.get('items', [])):
                    operation_result = item.get('index') or item.get('create')
                    if operation_result and operation_result.get('status', 0) >= 400:
                        error_count += 1
                        error_details = operation_result.get('error', {})
                        logger.error(f"  - L·ªói document #{i+1} (ID: {operation_result.get('_id', 'N/A')}): Status={operation_result.get('status', 'N/A')}, Type={error_details.get('type', 'N/A')}, Reason={error_details.get('reason', 'N/A')}")
                logger.error(f"T·ªïng c·ªông {error_count} l·ªói trong {len(operations)//2} documents.")
            else:
                logger.info("Bulk indexing ho√†n t·∫•t th√†nh c√¥ng.")
        except ConnectionTimeout: # B·∫Øt l·ªói Timeout c·ª• th·ªÉ c·ªßa client
             logger.error("L·ªói: Timeout khi th·ª±c hi·ªán bulk index. Th·ª≠ tƒÉng request_timeout trong client.bulk().")
        except Exception as e:
            logger.error(f"L·ªói kh√¥ng x√°c ƒë·ªãnh trong qu√° tr√¨nh bulk indexing: {e}", exc_info=True)


    logger.info("ƒê√£ ƒë√≥ng k·∫øt n·ªëi Elasticsearch (do d√πng async with).")

# --- Main execution block (gi·ªØ nguy√™n) ---
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Migrate OCR data from JSON to Elasticsearch.")
    parser.add_argument("--file_path", type=str, required=True, help="Path to the JSON file containing OCR data (format: {\"key\": \"text\"}).")
    args = parser.parse_args()

    try:
        es_settings = ElasticsearchSettings()
        asyncio.run(migrate_ocr_data(args.file_path, es_settings))
    except ConnectionError as conn_err:
         logger.critical(f"Script b·ªã d·ª´ng do l·ªói k·∫øt n·ªëi: {conn_err}")
         sys.exit(1)
    except Exception as main_error:
         logger.critical(f"Script b·ªã d·ª´ng do l·ªói nghi√™m tr·ªçng: {main_error}", exc_info=True)
         sys.exit(1)